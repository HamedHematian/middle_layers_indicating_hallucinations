{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take LLaVA-1.5-7B as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from model_manager import ModelManager\n",
    "from utils import setup_seeds, disable_torch_init\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Case studies on LVLMs.\")\n",
    "parser.add_argument(\"--model\", type=str, default='llava-1.5', help=\"model\")\n",
    "parser.add_argument(\n",
    "    \"--data-path\",\n",
    "    type=str,\n",
    "    default=\"/path/to/COCO/val2014\",\n",
    "    help=\"data path\",\n",
    ")\n",
    "parser.add_argument(\"--batch-size\", type=int, default=1)\n",
    "parser.add_argument(\"--beam\", type=int, default=1) # 1 for Greedy Decoding\n",
    "# parser.add_argument(\"--sample\", action=\"store_true\")\n",
    "parser.add_argument(\"--max-tokens\", type=int, default=512)\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "setup_seeds()\n",
    "disable_torch_init()\n",
    "\n",
    "# Load model\n",
    "model_manager = ModelManager(args.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAR Distribution and Logit Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize CHAIR evaluator\n",
    "from chair_utils import CHAIR\n",
    "cache = 'chair.pkl'\n",
    "if cache and os.path.exists(cache):\n",
    "    evaluator = pickle.load(open(cache, 'rb'))\n",
    "    print(f\"loaded evaluator from cache: {cache}\")\n",
    "else:\n",
    "    print(f\"cache not setted or not exist yet, building from scratch...\")\n",
    "    evaluator = CHAIR('/path/to/COCO/annotations')\n",
    "    pickle.dump(evaluator, open(cache, 'wb'))\n",
    "    print(f\"cached evaluator to: {cache}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chair_utils import chair_eval\n",
    "from utils import set_act_get_hooks, remove_hooks\n",
    "from utils import get_only_attn_out_contribution\n",
    "from utils import attnw_over_vision_layer_head_selected_text\n",
    "\n",
    "img_query_lists = [\n",
    "    json.loads(line) for line in open('./examples/toy_img_query_list.jsonl')\n",
    "]\n",
    "real_attn_contribution_across_layers = []\n",
    "visual_attn_weights = []\n",
    "real_SVAR_5_18 = []\n",
    "hallu_SVAR_5_18 = []\n",
    "\n",
    "for img_query in tqdm(img_query_lists):\n",
    "    # prepare inputs\n",
    "    img_id = f\"COCO_val2014_{str(img_query['image_id']).zfill(12)}.jpg\"\n",
    "    img_path = os.path.join(args.data_path, img_id)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = model_manager.image_processor(img, return_tensors=\"pt\")\n",
    "    img['pixel_values'] = img['pixel_values'].unsqueeze(0)\n",
    "\n",
    "    query = [img_query['instruction']]\n",
    "    questions, input_ids, kwargs = model_manager.prepare_inputs_for_model(query, img)\n",
    "\n",
    "    # use hooks to get the attention sublayers' output\n",
    "    hooks = set_act_get_hooks(model_manager.llm_model.model, attn_out=True)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model_manager.llm_model.generate(\n",
    "            input_ids,\n",
    "            do_sample=False,\n",
    "            num_beams=args.beam,\n",
    "            max_new_tokens=args.max_tokens,\n",
    "            use_cache=True,\n",
    "            output_scores=True,\n",
    "            output_hidden_states=True,\n",
    "            output_attentions=True,\n",
    "            return_dict_in_generate=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "    remove_hooks(hooks)\n",
    "\n",
    "    answer = model_manager.tokenizer.batch_decode(outputs['sequences'], skip_special_tokens=True)[0].strip()\n",
    "    img_info = chair_eval(evaluator, img_id, answer)\n",
    "\n",
    "    # calc some constants\n",
    "    vision_token_start = model_manager.img_start_idx\n",
    "    vision_token_end = model_manager.img_end_idx\n",
    "    input_token_len = (model_manager.llm_model.get_vision_tower().num_patches\n",
    "                    + len(input_ids[0]) - 1 # -1 for the <image> token\n",
    "    )\n",
    "    gt_words = img_info['mscoco_gt_words']\n",
    "    generated_words = img_info['mscoco_generated_words']\n",
    "\n",
    "    # Real words Calculation\n",
    "    for ri, real_word in enumerate(set(generated_words) & set(gt_words)):\n",
    "        # calculate attn sublayer contribution for each real word\n",
    "        try:\n",
    "            # get attn sublayer contribution\n",
    "            _records = get_only_attn_out_contribution(\n",
    "                model_manager.llm_model, model_manager.tokenizer,\n",
    "                outputs, real_word, input_token_len,\n",
    "            )\n",
    "            real_attn_contribution_across_layers.append(_records)\n",
    "\n",
    "            # get visual attention weights\n",
    "            real_word_attnw_matrix, _ = attnw_over_vision_layer_head_selected_text(\n",
    "                    real_word, outputs, model_manager.tokenizer,\n",
    "                    vision_token_start, vision_token_end\n",
    "            )\n",
    "            visual_attn_weights.append(real_word_attnw_matrix)\n",
    "            real_word_layer_attnw = real_word_attnw_matrix.mean(axis=1)[::-1]\n",
    "            real_SVAR_5_18.append(real_word_layer_attnw[5:19].sum())\n",
    "        except:\n",
    "            print(f\"'{real_word}' not found in the generated text.\")\n",
    "\n",
    "    if len(img_info['mscoco_hallucinated_words']) == 0:\n",
    "        continue\n",
    "\n",
    "    # Hallucinated words Calculation\n",
    "    hallucination_words = [\n",
    "        item for sublist in img_info['mscoco_hallucinated_words'] for item in sublist\n",
    "    ]\n",
    "    for hi, hallu_word in enumerate(set(hallucination_words)):\n",
    "        # calculate attn sublayer contribution for each hallu word\n",
    "        try:\n",
    "            # get visual attention weights\n",
    "            hallu_word_attnw_matrix, _ = attnw_over_vision_layer_head_selected_text(\n",
    "                    hallu_word, outputs, model_manager.tokenizer,\n",
    "                    vision_token_start, vision_token_end\n",
    "            )\n",
    "            visual_attn_weights.append(hallu_word_attnw_matrix)\n",
    "            hallu_word_layer_attnw = hallu_word_attnw_matrix.mean(axis=1)[::-1]\n",
    "            hallu_SVAR_5_18.append(hallu_word_layer_attnw[5:19].sum())\n",
    "        except:\n",
    "            print(f\"'{hallu_word}' not found in the generated text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attention map plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import plot_VAR_heatmap\n",
    "avg_visual_attn_weights = np.array(visual_attn_weights).mean(axis=0)\n",
    "# sort heads\n",
    "sorted_idx = np.argsort(-avg_visual_attn_weights, axis=-1)\n",
    "avg_data = np.take_along_axis(avg_visual_attn_weights, sorted_idx, axis=-1)\n",
    "\n",
    "# plot heatmap\n",
    "fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
    "im = axes.imshow(\n",
    "    avg_data, vmin=avg_data.min(),\n",
    "    vmax=avg_data.max(), cmap='Blues'\n",
    ")\n",
    "n_layer, n_head = avg_data.shape\n",
    "y_label_list = [str(i) for i in range(n_layer)]\n",
    "axes.set_yticks(np.arange(0, n_layer, 2))\n",
    "axes.set_yticklabels(y_label_list[::-1][::2])\n",
    "axes.set_xlabel(\"Sorted Heads\")\n",
    "axes.set_ylabel(\"Layers\")\n",
    "fig.colorbar(im, ax=axes, shrink=0.4, location='bottom')\n",
    "plt.xticks([])\n",
    "# plt.savefig(\"VAR Distribution\", dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attention sublayers contribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_real_attn_contri = torch.tensor(real_attn_contribution_across_layers)\n",
    "prob_contribution = tensor_real_attn_contri.mean(dim=0).numpy()\n",
    "_, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.plot(np.arange(-1, len(prob_contribution) + 1),\n",
    "        [0] * len(prob_contribution) + [0, 0],\n",
    "        linestyle=\"--\", color=\"#AAABA8\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "ax.plot(\n",
    "    prob_contribution,\n",
    "    marker=\"o\", color='#2A7AB9',\n",
    "    label=\"Real Words\",\n",
    "    markersize=4,\n",
    "    )\n",
    "std = tensor_real_attn_contri.std(dim=0).numpy()\n",
    "ax.fill_between(\n",
    "    range(len(prob_contribution)),\n",
    "    prob_contribution - std,\n",
    "    prob_contribution + std,\n",
    "    color=\"#66AAD2\",\n",
    "    alpha=0.25\n",
    ")\n",
    "ax.set_title(\n",
    "    f\"Mean Prob. Contribution of Attn Module with {len(real_attn_contribution_across_layers)} Real Objects\",\n",
    "    fontsize=10\n",
    ")\n",
    "ax.set_xlabel(\"Layer\", fontsize=12)\n",
    "ax.set_ylabel(\"Logit\", fontsize=12)\n",
    "ax.grid(alpha=0.9, linestyle='--', color='#AAAAAA')\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.xlim(-1, len(prob_contribution))\n",
    "plt.subplots_adjust(left=0.2, right=0.85, top=0.85, bottom=0.15)\n",
    "# plt.savefig(\"attn_contribution.pdf\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\text{SVAR}_{5\\text{-}18}$ comparison of real and hallucinated object tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3), dpi=300)\n",
    "sns.boxplot(\n",
    "    data=[real_SVAR_5_18, hallu_SVAR_5_18],\n",
    "    medianprops = {'linestyle':'--','color':'white', 'linewidth':1.5},\n",
    "    meanprops = {'marker':'o',\n",
    "                'markerfacecolor':'#C00000',\n",
    "                'markeredgecolor':'black',\n",
    "                'markersize':6},\n",
    "    fliersize=4,\n",
    "    linewidth=2,\n",
    "    saturation=0.8,\n",
    "    showfliers=True,\n",
    "    showmeans=True\n",
    "    )\n",
    "plt.title(f\"Sumed Attn in layers 5~18 with total {len(real_SVAR_5_18)} pairs\", fontsize=10)\n",
    "plt.xticks([0, 1], ['Real', 'Hallucinated'])\n",
    "plt.ylabel(\"Sumed attention weights\")\n",
    "plt.xlabel(\"Object token type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative results of $\\text{SVAR}_{5\\text{-}18}$ for detecting hallucinated object tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ROC curve\n",
    "y_scores = np.concatenate([real_SVAR_5_18, hallu_SVAR_5_18])\n",
    "y_true = np.concatenate([np.ones(len(real_SVAR_5_18)), \n",
    "                        np.zeros(len(hallu_SVAR_5_18))])\n",
    "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(4, 3), dpi=300)\n",
    "sns.lineplot(\n",
    "    x=fpr, y=tpr, color='#5C73A2', lw=2, label=f'Our (AUROC = {roc_auc:.2f})'\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the text token via logit lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.logits_process import LogitsProcessorList, TopKLogitsWarper\n",
    "from utils import logitLens_of_vision_tokens_with_discrete_range\n",
    "\n",
    "img_path = os.path.join(args.data_path, f'COCO_val2014_000000499775.jpg')\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "query = [\"Please help me describe the image in detail.\"]\n",
    "questions, input_ids, kwargs = model_manager.prepare_inputs_for_model(query, img)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model_manager.llm_model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        num_beams=args.beam,\n",
    "        max_new_tokens=args.max_tokens,\n",
    "        use_cache=True,\n",
    "        output_scores=True,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "        return_dict_in_generate=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "discrete_range = [\n",
    "    [391, 396], # 1\n",
    "    [415, 420], # 1\n",
    "    [328, 330], # 2\n",
    "    [352, 354], # 2\n",
    "    [376, 378], # 2\n",
    "    [379, 382], # 3\n",
    "    [403, 406], # 3\n",
    "]\n",
    "range_id = [1, 1, 2, 2, 2, 3, 3] # range_id代表第几个区域\n",
    "layer_range = [0, 5, 7, 10, 12, 15, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 30, 31]\n",
    "logits_warper = TopKLogitsWarper(top_k=50, filter_value=float('-inf'))\n",
    "logits_processor = LogitsProcessorList([])\n",
    "\n",
    "logitLens_of_vision_tokens_with_discrete_range(\n",
    "    model_manager.llm_model, model_manager.tokenizer, input_ids, outputs,\n",
    "    model_manager.img_start_idx, discrete_range,\n",
    "    layer_range,\n",
    "    logits_warper, logits_processor,\n",
    "    savefig=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
